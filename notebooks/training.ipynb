{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vunz/Projects/Diploma/jup/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from helpers import get_all_metadata, handle_empty\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.datasets import CitationFull\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.utils.data import Dataset, DataLoader as TorchDataLoader\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATRegression(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, heads=4):\n",
    "        super(GATRegression, self).__init__()\n",
    "        self.gat1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=0.6)\n",
    "        self.gat2 = GATConv(hidden_channels * heads, out_channels, heads=1, concat=False)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = pickle.load(open(\"../data/graph_medium.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17778/17778 [6:07:48<00:00,  1.24s/it]  \n"
     ]
    }
   ],
   "source": [
    "node_labels = dict()\n",
    "for node in tqdm(graph.nodes):\n",
    "    metadata = get_all_metadata(node)\n",
    "    metadata[\"authors\"] = ','.join(metadata[\"authors\"]) if metadata[\"authors\"] and len(metadata[\"authors\"]) > 1 else metadata[\"authors\"]\n",
    "    node_labels[node] = metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in node_labels:\n",
    "    if node_labels[elem][\"authors\"]:\n",
    "        if len(node_labels[elem][\"authors\"]) == 1:\n",
    "            node_labels[elem][\"authors\"] = node_labels[elem][\"authors\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [handle_empty(node_labels[obj][\"title\"]) for obj in node_labels]\n",
    "authors = [handle_empty(node_labels[obj][\"authors\"]) for obj in node_labels]\n",
    "venues = [handle_empty(node_labels[obj][\"venue\"]) for obj in node_labels]\n",
    "years = [node_labels[obj][\"year\"][0] if node_labels[obj][\"year\"] and node_labels[obj][\"year\"][0] else 0 for obj in node_labels]\n",
    "abstracts = [handle_empty(node_labels[obj][\"abstract\"]) for obj in node_labels]\n",
    "keywords = [handle_empty(node_labels[obj][\"keywords\"]) for obj in node_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10430"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"years.csv\", years, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17778, 1538)\n"
     ]
    }
   ],
   "source": [
    "title_embeddings = sentence_model.encode(titles)\n",
    "author_embeddings = sentence_model.encode(authors)\n",
    "venue_embeddings = sentence_model.encode(venues)\n",
    "abstract_embeddings = sentence_model.encode(abstracts)\n",
    "keyword_embeddings = tfidf_vectorizer.fit_transform(keywords).toarray()\n",
    "years_normalized = scaler.fit_transform(np.array(years).reshape(-1, 1))\n",
    "\n",
    "encoded_data = np.hstack([\n",
    "    title_embeddings,\n",
    "    author_embeddings,\n",
    "    venue_embeddings,\n",
    "    years_normalized,\n",
    "    abstract_embeddings,\n",
    "    keyword_embeddings\n",
    "])\n",
    "print(encoded_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_no_year = np.hstack([\n",
    "    title_embeddings,\n",
    "    author_embeddings,\n",
    "    venue_embeddings,\n",
    "    abstract_embeddings,\n",
    "    keyword_embeddings\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"labels_no_year.csv\", encoded_data_no_year, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"labels.csv\", encoded_data, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = np.genfromtxt(\"labels2.csv\", delimiter=',')\n",
    "encoded_data_no_year = np.genfromtxt(\"labels_no_year2.csv\", delimiter=',')\n",
    "years = np.genfromtxt(\"years2.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "years=np.genfromtxt(\"years.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1105.0269434132074\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(np.array(years)))\n",
    "print((np.array(years) == 2016).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17778, 1538)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_list = list(graph.nodes())\n",
    "node_mapping = {node: idx for idx, node in enumerate(node_list)}\n",
    "graph = nx.relabel_nodes(graph, node_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_dict = dict(enumerate(years))\n",
    "nx.set_node_attributes(graph, years_dict, name=\"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import from_networkx\n",
    "\n",
    "pyg_data = from_networkx(graph.to_undirected())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(g: nx.Graph, graph, node_labels, target_function):\n",
    "    x = torch.tensor(node_labels, dtype=torch.float)\n",
    "    edge_index = graph.edge_index\n",
    "    y = torch.tensor(list(target_function(g).values()), dtype=torch.float)\n",
    "    return Data(x=x, edge_index=edge_index, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pr = create_dataset(graph, pyg_data, encoded_data, nx.pagerank).to(device)\n",
    "dataset_harmonic = create_dataset(graph, pyg_data, encoded_data, nx.harmonic_centrality).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_nodes = [n for n, attr in graph.nodes(data=True) if attr.get('year') == 2016]\n",
    "train_nodes = list(set(list(range(len(graph.nodes)))).difference(set(test_nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = (torch.zeros(len(graph.nodes), dtype=torch.bool).to(device))\n",
    "train_mask[train_nodes] = True\n",
    "test_mask = (torch.zeros(len(graph.nodes), dtype=torch.bool).to(device))\n",
    "test_mask[test_nodes] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GATRegression(in_channels=1538, hidden_channels=2048, out_channels=1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, data, model, optimizer, loss_fn, train_mask, test_mask):\n",
    "    for epoch in trange(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index).squeeze()\n",
    "        loss = loss_fn(out[train_mask], data.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Loss: {loss.item():.4f}\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(data.x, data.edge_index).squeeze()\n",
    "        test_loss = loss_fn(predictions[test_mask], data.y[test_mask])\n",
    "        mape = mean_absolute_percentage_error(np.array(data.y[test_mask]), np.array(predictions[test_mask]))\n",
    "        print(f'Test Loss: MSE = {test_loss.item()}, MAPE = {mape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:20<03:03, 20.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:59<00:00, 11.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 24.6703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: MSE = 89.1614761352539, MAPE = 167157.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22087/2881573262.py:15: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  mape = mean_absolute_percentage_error(np.array(data.y[test_mask]), np.array(predictions[test_mask]))\n"
     ]
    }
   ],
   "source": [
    "train(10, dataset_pr, model, optimizer, loss_fn, train_mask, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.9862e-06)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(dataset_pr.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:10<01:32, 10.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 211.6375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:14<00:00, 13.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 75.7704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: MSE = 107.71257781982422, MAPE = 5.880670547485352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22087/2881573262.py:15: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  mape = mean_absolute_percentage_error(np.array(data.y[test_mask]), np.array(predictions[test_mask]))\n"
     ]
    }
   ],
   "source": [
    "train(10, dataset_harmonic, model, optimizer, loss_fn, train_mask, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.6315)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(dataset_harmonic.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second model using time series predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, heads):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads)\n",
    "        self.conv2 = GATConv(hidden_dim*heads, hidden_dim, heads=1)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.elu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphTemporalPipeline(nn.Module):\n",
    "    def __init__(self, gat_encoder, temporal_model):\n",
    "        super().__init__()\n",
    "        self.gat_encoder = gat_encoder\n",
    "        self.temporal_model = temporal_model\n",
    "        \n",
    "    def forward(self, graphs):\n",
    "        # Encode each graph\n",
    "        embeddings = []\n",
    "        for graph in graphs:\n",
    "            emb = self.gat_encoder(graph.x, graph.edge_index)\n",
    "            emb = emb.mean(dim=0)\n",
    "            embeddings.append(emb)\n",
    "        \n",
    "        # Stack embeddings over time [num_years, num_nodes, embedding_dim]\n",
    "        temporal_input = torch.stack(embeddings, dim=0).unsqueeze(0)\n",
    "        \n",
    "        # Predict centrality measures\n",
    "        return self.temporal_model(temporal_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = encoded_data_no_year.shape[0]\n",
    "num_features = encoded_data_no_year.shape[1]\n",
    "hidden_dim = 2048\n",
    "num_heads = 1\n",
    "years_range = list(range(2001, 2015))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat = GATEncoder(num_features, hidden_dim, num_heads)\n",
    "temporal = TemporalModel(hidden_dim, hidden_dim, num_nodes)\n",
    "pipeline = GraphTemporalPipeline(gat, temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(year, graph, encodings, target_func):\n",
    "    filtered_nodes = [n for n, attr in graph.nodes(data=True) if 0 < attr.get('year') <= year]\n",
    "    filtered_edges = [(u, v) for u,v in graph.edges if u in filtered_nodes and v in filtered_nodes]\n",
    "    subgraph = nx.Graph()\n",
    "    subgraph.add_nodes_from(graph.nodes)\n",
    "    subgraph.add_edges_from(filtered_edges)\n",
    "    # subgraph = graph.subgraph(filtered_nodes)\n",
    "    pyg_data = from_networkx(subgraph)\n",
    "    x = torch.tensor(np.array(encodings), dtype=torch.float)\n",
    "    edge_indices = pyg_data.edge_index\n",
    "    y = torch.tensor(list(target_func(subgraph).values()), dtype=torch.float)\n",
    "    return Data(x=x, edge_index=edge_indices, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = [load_graph(year, graph, encoded_data_no_year, nx.pagerank) for year in years_range]\n",
    "sequences = []\n",
    "for i in range(len(graphs) - 1):\n",
    "    sequence = graphs[:i+1]\n",
    "    target = graphs[i+1]\n",
    "    sequences.append((sequence, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_harmonic = [load_graph(year, graph, encoded_data_no_year, nx.harmonic_centrality) for year in years_range]\n",
    "sequences_harmonic = []\n",
    "for i in range(len(graphs_harmonic) - 1):\n",
    "    sequence = graphs_harmonic[:i+1]\n",
    "    target = graphs_harmonic[i+1]\n",
    "    sequences_harmonic.append((sequence, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(pipeline.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_test = YearlyGraphDataset(years_range.append(2025), graph, encoded_data, nx.pagerank)\n",
    "# historical_graphs = dataset_test.graphs[:-1]\n",
    "# test_subgraph = dataset_test.graphs[-1]\n",
    "# historical_embeds = [gat(g.x, g.edge_index) for g in historical_graphs]\n",
    "# test_embed = gat(test_subgraph.x, test_subgraph.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(epochs, seqs, model, optimizer, criterion):\n",
    "    for _ in trange(epochs):\n",
    "        for sequence, target in seqs:\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(sequence)\n",
    "            loss = criterion(preds, target.y.unsqueeze(0))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(graph, target_func, model, dataset, loss_fn):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         filtered_nodes = [n for n, attr in graph.nodes(data=True) if 0 < attr.get('year') <= 2025]\n",
    "#         subgraph = graph.subgraph(filtered_nodes)\n",
    "#         target_values = np.array(target_func(subgraph).values())[filtered_nodes]\n",
    "#         predictions = model(dataset.x, dataset.edge_index).squeeze()[filtered_nodes]\n",
    "#         res = loss_fn(predictions, target_values)\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "train_lstm(10, sequences, pipeline, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on test = 6.088639281642827e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vunz/Projects/Diploma/jup/lib/python3.12/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([10430])) that is different to the input size (torch.Size([1, 10430])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "graphs.append(load_graph(2016, graph, encoded_data_no_year, nx.pagerank))\n",
    "sequences.append([graphs[:-1], graphs[-1]])\n",
    "pipeline.eval()\n",
    "with torch.no_grad():\n",
    "    preds = pipeline(sequences[-1][0])\n",
    "    loss = criterion(preds, sequences[-1][1].y)\n",
    "    print(f\"Loss on test = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0005)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(sequences[-1][1].y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10430"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [07:33<4:54:54, 453.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 34592.9062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [13:37<4:13:45, 400.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 29877.0098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [18:41<3:39:50, 356.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 25632.2969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [23:36<3:19:24, 332.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 21923.3477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [28:33<3:06:22, 319.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 18709.4805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [33:33<2:57:17, 312.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 15932.3525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [37:53<2:42:35, 295.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 13540.8594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [42:11<2:31:21, 283.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11484.5693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 9/40 [46:29<2:22:29, 275.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 9720.4004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 10/40 [50:48<2:15:11, 270.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 8210.2832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/40 [55:06<2:08:54, 266.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6920.6084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 12/40 [59:24<2:03:16, 264.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5821.7866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 13/40 [1:03:42<1:57:57, 262.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4887.8145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 14/40 [1:07:59<1:52:58, 260.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4095.8777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 15/40 [1:12:16<1:48:10, 259.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3426.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [1:16:34<1:43:33, 258.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2860.7917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 17/40 [1:20:51<1:39:02, 258.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2385.0676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 18/40 [1:25:08<1:34:34, 257.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1985.6947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 19/40 [1:29:25<1:30:10, 257.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1651.3024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [1:33:42<1:25:51, 257.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1372.0796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 21/40 [1:37:59<1:21:30, 257.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1139.5837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 22/40 [1:42:16<1:17:10, 257.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 946.5646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 23/40 [1:46:33<1:12:51, 257.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 786.8124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [1:50:52<1:08:45, 257.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 655.0202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 25/40 [1:55:17<1:04:57, 259.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 546.6632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 26/40 [1:59:35<1:00:30, 259.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 457.8926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 27/40 [2:03:52<56:03, 258.69s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 385.4426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 28/40 [2:08:09<51:38, 258.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 326.5485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 29/40 [2:12:31<47:32, 259.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 278.8766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 30/40 [2:16:59<43:38, 261.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 240.4620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 31/40 [2:21:21<39:15, 261.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 209.6553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 32/40 [2:25:39<34:47, 260.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 185.0765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 33/40 [2:29:58<30:22, 260.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 165.5743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 34/40 [2:34:17<25:59, 259.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 150.1922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 35/40 [2:38:37<21:39, 259.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 138.1382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 36/40 [2:42:56<17:18, 259.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 128.7590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 37/40 [2:47:15<12:58, 259.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 121.5185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 38/40 [2:51:34<08:38, 259.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 115.9784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 39/40 [2:55:52<04:19, 259.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 111.7819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [3:00:11<00:00, 270.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 108.6403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_lstm(10, sequences_harmonic, pipeline, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on test = 137.4388427734375\n"
     ]
    }
   ],
   "source": [
    "graphs_harmonic.append(load_graph(2016, graph, encoded_data_no_year, nx.harmonic_centrality))\n",
    "sequences_harmonic.append([graphs_harmonic[:-1], graphs_harmonic[-1]])\n",
    "pipeline.eval()\n",
    "with torch.no_grad():\n",
    "    preds = pipeline(sequences_harmonic[-1][0])\n",
    "    loss = criterion(preds, sequences_harmonic[-1][1].y.unsqueeze(0))\n",
    "    print(f\"Loss on test = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(163.2631)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(sequences_harmonic[-1][1].y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можем заметить, вторая модель предсказывает неплохо хотя бы относительно среднего отклонения.\n",
    "\n",
    "Но для чистоты эскперимента непосредственно с моделями, хотелось бы проверить их работу на открытых датасетах и сравнить результат со статьей про DGINI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потом я понял, что они решали немного другую задачи, и мне пришлось бы слишком сильно менять структуру модели, чтобы это учесть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import AMiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://www.dropbox.com/s/1bnz8r7mofx0osf/net_aminer.zip?dl=1\n",
      "Extracting ../data/AMiner_dataset/net_aminer.zip\n",
      "Downloading https://www.dropbox.com/s/nkocx16rpl4ydde/label.zip?dl=1\n",
      "Extracting ../data/AMiner_dataset/raw/label.zip\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = AMiner(root=\"../data/AMiner_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  author={\n",
       "    y=[246678],\n",
       "    y_index=[246678],\n",
       "    num_nodes=1693531,\n",
       "  },\n",
       "  venue={\n",
       "    y=[134],\n",
       "    y_index=[134],\n",
       "    num_nodes=3883,\n",
       "  },\n",
       "  paper={ num_nodes=3194405 },\n",
       "  (paper, written_by, author)={ edge_index=[2, 9323605] },\n",
       "  (author, writes, paper)={ edge_index=[2, 9323605] },\n",
       "  (paper, published_in, venue)={ edge_index=[2, 3194405] },\n",
       "  (venue, publishes, paper)={ edge_index=[2, 3194405] }\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/abdullahdekebobeketa/aminer-citation-ntk-dataset-v11?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3.75G/3.75G [02:45<00:00, 24.3MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/vunz/.cache/kagglehub/datasets/abdullahdekebobeketa/aminer-citation-ntk-dataset-v11/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"abdullahdekebobeketa/aminer-citation-ntk-dataset-v11\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "counter = 0\n",
    "with open(\"../data/dblp_papers_v11.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "        counter += 1\n",
    "        if counter == 100000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'title', 'authors', 'venue', 'year', 'n_citation', 'page_start', 'page_end', 'doc_type', 'publisher', 'volume', 'issue', 'doi', 'references', 'indexed_abstract', 'fos'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = CitationFull(root=\"../data/datasets\", name=\"dblp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[17716, 1639], edge_index=[2, 105734], y=[17716])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data._data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.load(\"../data/datasets/dblp/raw/dblp.npz\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adj_data',\n",
       " 'attr_indices',\n",
       " 'adj_indices',\n",
       " 'adj_shape',\n",
       " 'adj_indptr',\n",
       " 'attr_data',\n",
       " 'idx_to_attr',\n",
       " 'attr_indptr',\n",
       " 'labels',\n",
       " 'attr_shape',\n",
       " 'idx_to_node',\n",
       " 'attr_text']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrs = []\n",
    "for file in dataset.files:\n",
    "    arrs.append(dataset[file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25524727, 0.57424884, 0.41145328, ..., 0.34304865, 0.2042381 ,\n",
       "       0.48243084], shape=(92192,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arrs[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jup",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
